"""
T√¢ches Celery pour le module Insights.

Fournit les t√¢ches asynchrones pour :
- Agr√©gation p√©riodique des m√©triques
- G√©n√©ration de rapports automatiques
- Nettoyage des donn√©es anciennes
- Collecte de m√©triques syst√®me
"""
import logging
from datetime import datetime, timedelta
from django.utils import timezone
from celery import shared_task
from django.db.models import Count, Avg, Sum
from django.core.cache import cache

from .models import UserActivity, SystemMetric, ApplicationMetric, UserMetric
from .services import MetricsCollector, AnalyticsService

logger = logging.getLogger(__name__)
print("üîç DEBUG: Loading insights/tasks.py module")

@shared_task(bind=True, max_retries=3)
def collect_system_metrics_task(self):
    """
    T√¢che p√©riodique pour collecter les m√©triques syst√®me.

    Cette t√¢che est ex√©cut√©e r√©guli√®rement pour surveiller
    les performances du syst√®me.
    """
    try:
        logger.info("D√©but de la collecte des m√©triques syst√®me")

        # Collecter les m√©triques syst√®me
        MetricsCollector.collect_system_metrics()

        logger.info("Collecte des m√©triques syst√®me termin√©e avec succ√®s")
        return {'status': 'success', 'message': 'M√©triques syst√®me collect√©es'}

    except Exception as e:
        logger.error(f"Erreur lors de la collecte des m√©triques syst√®me: {str(e)}", exc_info=True)

        # Retry avec backoff exponentiel
        if self.request.retries < self.max_retries:
            countdown = 60 * (2 ** self.request.retries)  # 1min, 2min, 4min
            raise self.retry(exc=e, countdown=countdown)

        return {
            'status': 'error',
            'message': f'√âchec de la collecte apr√®s {self.max_retries} tentatives',
            'error': str(e)
        }

@shared_task(bind=True, max_retries=3)
def aggregate_daily_metrics_task(self, date_str=None):
    """
    T√¢che pour agr√©ger les m√©triques quotidiennes.

    Regroupe les m√©triques par jour pour am√©liorer les performances
    des requ√™tes d'analytics.
    """
    try:
        # Utiliser la date du jour si non sp√©cifi√©e
        if date_str:
            target_date = datetime.strptime(date_str, '%Y-%m-%d').date()
        else:
            target_date = timezone.now().date() - timedelta(days=1)  # Jour pr√©c√©dent

        logger.info(f"Agr√©gation des m√©triques pour le {target_date}")

        # Agr√©ger les activit√©s utilisateur par organisation et type
        activities = UserActivity.objects.filter(
            created_at__date=target_date
        ).values('organization', 'activity_type').annotate(count=Count('id'))

        for activity in activities:
            if activity['organization']:
                # Cr√©er ou mettre √† jour la m√©trique agr√©g√©e
                UserMetric.objects.update_or_create(
                    organization_id=activity['organization'],
                    metric_type=f"daily.{activity['activity_type']}",
                    date=target_date,
                    defaults={'value': activity['count']}
                )

        # Agr√©ger les m√©triques syst√®me
        system_metrics = SystemMetric.objects.filter(
            created_at__date=target_date
        ).values('metric_type').annotate(
            avg_value=Avg('value'),
            count=Count('id')
        )

        # Stocker les m√©triques agr√©g√©es dans le cache pour acc√®s rapide
        cache_key = f"daily_metrics_{target_date.strftime('%Y%m%d')}"
        cache_data = {
            'date': target_date,
            'activities_count': activities.count(),
            'system_metrics': {
                item['metric_type']: {
                    'avg': round(item['avg_value'], 2),
                    'count': item['count']
                } for item in system_metrics
            }
        }

        # Cache pour 7 jours
        cache.set(cache_key, cache_data, 7 * 24 * 3600)

        logger.info(f"Agr√©gation termin√©e pour {target_date}")
        return {'status': 'success', 'aggregated_records': len(activities)}

    except Exception as e:
        logger.error(f"Erreur lors de l'agr√©gation des m√©triques: {str(e)}", exc_info=True)

        if self.request.retries < self.max_retries:
            countdown = 300 * (2 ** self.request.retries)  # 5min, 10min, 20min
            raise self.retry(exc=e, countdown=countdown)

        return {
            'status': 'error',
            'message': f'√âchec de l\'agr√©gation apr√®s {self.max_retries} tentatives',
            'error': str(e)
        }

@shared_task(bind=True, max_retries=3)
def generate_analytics_reports_task(self, organization_id=None):
    """
    T√¢che pour g√©n√©rer automatiquement les rapports d'analytics.

    G√©n√®re des rapports p√©riodiques pour toutes les organisations
    ou une organisation sp√©cifique.
    """
    try:
        logger.info("D√©but de la g√©n√©ration des rapports d'analytics")

        # D√©terminer les organisations √† traiter
        if organization_id:
            from apps.foundation.models import Organization
            try:
                organization = Organization.objects.get(id=organization_id)
                organizations = [organization]
            except Organization.DoesNotExist:
                return {'status': 'error', 'message': 'Organisation non trouv√©e'}
        else:
            from apps.foundation.models import Organization
            organizations = Organization.objects.filter(is_active=True)

        reports_generated = 0

        for organization in organizations:
            try:
                # G√©n√©rer le rapport pour les 30 derniers jours
                end_date = timezone.now().date()
                start_date = end_date - timedelta(days=30)

                report = AnalyticsService.generate_analytics_report(
                    organization_id=str(organization.id),
                    start_date=start_date,
                    end_date=end_date,
                    metrics=['user_activity', 'system_performance', 'app_metrics'],
                    group_by='day'
                )

                # Sauvegarder le rapport dans le cache
                cache_key = f"analytics_report_{organization.id}_{end_date.strftime('%Y%m%d')}"
                cache.set(cache_key, report, 24 * 3600)  # Cache pour 24h

                reports_generated += 1

            except Exception as e:
                logger.error(f"Erreur lors de la g√©n√©ration du rapport pour {organization.name}: {str(e)}")
                continue

        logger.info(f"G√©n√©ration de rapports termin√©e: {reports_generated} rapports g√©n√©r√©s")
        return {
            'status': 'success',
            'reports_generated': reports_generated,
            'organizations_processed': len(organizations)
        }

    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration des rapports: {str(e)}", exc_info=True)

        if self.request.retries < self.max_retries:
            countdown = 600 * (2 ** self.request.retries)  # 10min, 20min, 40min
            raise self.retry(exc=e, countdown=countdown)

        return {
            'status': 'error',
            'message': f'√âchec de la g√©n√©ration apr√®s {self.max_retries} tentatives',
            'error': str(e)
        }

@shared_task(bind=True, max_retries=3)
def cleanup_old_metrics_task(self, days_to_keep=90):
    """
    T√¢che pour nettoyer les anciennes m√©triques.

    Supprime les donn√©es de m√©triques plus anciennes que
    la p√©riode de r√©tention sp√©cifi√©e.
    """
    try:
        logger.info(f"D√©but du nettoyage des m√©triques (r√©tention: {days_to_keep} jours)")

        cutoff_date = timezone.now() - timedelta(days=days_to_keep)

        # Compter les enregistrements avant suppression
        activities_before = UserActivity.objects.filter(created_at__lt=cutoff_date).count()
        system_metrics_before = SystemMetric.objects.filter(created_at__lt=cutoff_date).count()
        app_metrics_before = ApplicationMetric.objects.filter(created_at__lt=cutoff_date).count()
        user_metrics_before = UserMetric.objects.filter(created_at__lt=cutoff_date).count()

        # Supprimer les anciennes donn√©es
        deleted_activities = UserActivity.objects.filter(created_at__lt=cutoff_date).delete()[0]
        deleted_system_metrics = SystemMetric.objects.filter(created_at__lt=cutoff_date).delete()[0]
        deleted_app_metrics = ApplicationMetric.objects.filter(created_at__lt=cutoff_date).delete()[0]
        deleted_user_metrics = UserMetric.objects.filter(created_at__lt=cutoff_date).delete()[0]

        logger.info(
            "Nettoyage termin√©: "
            f"Activit√©s={deleted_activities}, "
            f"Syst√®me={deleted_system_metrics}, "
            f"Apps={deleted_app_metrics}, "
            f"Utilisateurs={deleted_user_metrics}"
        )

        return {
            'status': 'success',
            'deleted_records': {
                'activities': deleted_activities,
                'system_metrics': deleted_system_metrics,
                'app_metrics': deleted_app_metrics,
                'user_metrics': deleted_user_metrics,
            },
            'retention_days': days_to_keep
        }

    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des m√©triques: {str(e)}", exc_info=True)

        if self.request.retries < self.max_retries:
            countdown = 3600 * (2 ** self.request.retries)  # 1h, 2h, 4h
            raise self.retry(exc=e, countdown=countdown)

        return {
            'status': 'error',
            'message': f'√âchec du nettoyage apr√®s {self.max_retries} tentatives',
            'error': str(e)
        }

@shared_task(bind=True, max_retries=3)
def collect_application_performance_task(self, app_id=None):
    """
    T√¢che pour collecter les m√©triques de performance des applications.

    Cette t√¢che peut √™tre d√©clench√©e apr√®s le d√©ploiement
    ou ex√©cut√©e p√©riodiquement pour surveiller les performances.
    """
    try:
        logger.info("D√©but de la collecte des m√©triques d'application")

        from apps.runtime.models import GeneratedApp

        # Si un app_id sp√©cifique est fourni
        if app_id:
            try:
                app = GeneratedApp.objects.get(id=app_id)
                apps = [app]
            except GeneratedApp.DoesNotExist:
                return {'status': 'error', 'message': 'Application non trouv√©e'}
        else:
            # Toutes les applications actives
            apps = GeneratedApp.objects.filter(status='deployed')

        collected_metrics = 0

        for app in apps:
            try:
                # Collecter les m√©triques de base
                metrics_data = {
                    'response.time': {
                        'value': 200 + (hash(app.name) % 100),  # Simulation
                        'unit': 'ms'
                    },
                    'requests.count': {
                        'value': 100 + (hash(app.name) % 50),  # Simulation
                        'unit': 'count'
                    },
                    'errors.count': {
                        'value': (hash(app.name) % 10),  # Simulation
                        'unit': 'count'
                    },
                    'uptime': {
                        'value': 95 + (hash(app.name) % 5),  # Simulation
                        'unit': '%'
                    }
                }

                MetricsCollector.collect_application_metrics(app, metrics_data)
                collected_metrics += len(metrics_data)

            except Exception as e:
                logger.error(f"Erreur lors de la collecte des m√©triques pour {app.name}: {str(e)}")
                continue

        logger.info(f"Collecte des m√©triques d'application termin√©e: {collected_metrics} m√©triques collect√©es")
        return {
            'status': 'success',
            'apps_processed': len(apps),
            'metrics_collected': collected_metrics
        }

    except Exception as e:
        logger.error(f"Erreur lors de la collecte des m√©triques d'application: {str(e)}", exc_info=True)

        if self.request.retries < self.max_retries:
            countdown = 300 * (2 ** self.request.retries)  # 5min, 10min, 20min
            raise self.retry(exc=e, countdown=countdown)

        return {
            'status': 'error',
            'message': f'√âchec de la collecte apr√®s {self.max_retries} tentatives',
            'error': str(e)
        }


def start_periodic_tasks():
    """
    D√©marre les t√¢ches p√©riodiques lors du lancement de l'application.

    Cette fonction est appel√©e dans apps.py pour d√©marrer
    les t√¢ches r√©currentes.
    """
    try:
        # D√©marrer la collecte de m√©triques syst√®me toutes les 5 minutes
        collect_system_metrics_task.apply_async(
            countdown=300,  # D√©marrer dans 5 minutes
            expires=3600    # Expirer apr√®s 1 heure
        )

        # D√©marrer l'agr√©gation quotidienne √† minuit
        from datetime import time
        midnight = time(0, 0, 0)
        # Calculer le temps jusqu'√† minuit prochain
        now = timezone.now()
        midnight_today = now.replace(hour=0, minute=0, second=0, microsecond=0)
        if now > midnight_today:
            midnight_tomorrow = midnight_today + timedelta(days=1)
        else:
            midnight_tomorrow = midnight_today

        countdown_to_midnight = (midnight_tomorrow - now).total_seconds()

        aggregate_daily_metrics_task.apply_async(
            countdown=countdown_to_midnight,
            expires=86400  # Expirer apr√®s 24h
        )

        logger.info("T√¢ches p√©riodiques Insights d√©marr√©es")

    except Exception as e:
        logger.error(f"Erreur lors du d√©marrage des t√¢ches p√©riodiques: {str(e)}")
